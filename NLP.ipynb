{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fee024dd3d0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP \n",
    "# loading the language model instance\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'tutorial',\n",
       " 'is',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'in',\n",
       " 'spaCy',\n",
       " '.']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing a doc object \n",
    "# a doc object is a sequence of Token objects representing lexical token (wtf is a lexical token)\n",
    "# each token object has information about a particular piece of text\n",
    "\n",
    "intro_doc = nlp(\"This tutorial is about Natural Language Processing in spaCy.\")\n",
    "\n",
    "type(intro_doc)\n",
    "\n",
    "[token.text for token in intro_doc]\n",
    "\n",
    "# we called the .text attribute to get the text containsed within that token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto is a Python...\n",
      "He is interested in learning...\n"
     ]
    }
   ],
   "source": [
    "# Sentance Detection \n",
    "# the process of locating where sentances start and end in a given text\n",
    "# this allows you to divide a text into lingustically meaninful text \n",
    "\n",
    "# in spaCy the .sents property is used to extract setances from the Doc objects. \n",
    "\n",
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "\n",
    "about_doc = nlp(about_text)\n",
    "sentances = list(about_doc.sents)\n",
    "len(sentances)\n",
    "\n",
    "for sentance in sentances:\n",
    "    print(f\"{sentance[:5]}...\") # 5 indicates 4 words + ...\n",
    "\n",
    "# with .sents you get a list of Span objects representing individual sentances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    "# Tokens in spaCy \n",
    "# building the doc container involves tokenizing the text\n",
    "# tokenisation breaks a text down into basic units (tokens)\n",
    "\n",
    "for token in about_doc:\n",
    "    print(token, token.idx)\n",
    "\n",
    "# idx attribute represents the starting position of the token in the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n",
      "Gus                   True           False             False\n",
      "Proto                 True           False             False\n",
      "is                    True           False             True\n",
      "a                     True           False             True\n",
      "Python                True           False             False\n",
      "developer             True           False             False\n",
      "currently             True           False             False\n",
      "working               True           False             False\n",
      "for                   True           False             True\n",
      "a                     True           False             True\n",
      "London                True           False             False\n",
      "-                     False          True              False\n",
      "based                 True           False             False\n",
      "Fintech               True           False             False\n",
      "company               True           False             False\n",
      ".                     False          True              False\n",
      "He                    True           False             True\n",
      "is                    True           False             True\n",
      "interested            True           False             False\n",
      "in                    True           False             True\n",
      "learning              True           False             False\n",
      "Natural               True           False             False\n",
      "Language              True           False             False\n",
      "Processing            True           False             False\n",
      ".                     False          True              False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{'Text with Whitespace':22}\"\n",
    "    f\"{'Is Alphanumeric?':15}\"\n",
    "    f\"{'Is Punctuation?':18}\"\n",
    "    f\"{'Is Stop Word?'}\"\n",
    ")\n",
    "\n",
    "for token in about_doc:\n",
    "    print(\n",
    "        f\"{str(token.text_with_ws):22}\"\n",
    "        f\"{str(token.is_alpha):15}\"\n",
    "        f\"{str(token.is_punct):18}\"\n",
    "        f\"{str(token.is_stop)}\"\n",
    "    )\n",
    "\n",
    "# .text_with_ws prints the token along with any trailing space\n",
    "# .is_alpha is alphabetic characters or not \n",
    "# .is_punct is punctuation symbol or not \n",
    "# .is_stop is stopword or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "those\n",
      "move\n",
      "front\n",
      "our\n",
      "your\n",
      "seem\n",
      "sixty\n",
      "somehow\n",
      "would\n",
      "become\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "# in NLP stop words are generally removed because they arent significant \n",
    "\n",
    "spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stop_words)\n",
    "\n",
    "for stop_word in list(spacy_stop_words)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "# print a token in about_doc if it is not a stop word\n",
    "print([token for token in about_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is be\n",
      "helping help\n",
      "He he\n",
      "keeps keep\n",
      "organising organise\n",
      "meetups meetup\n",
      "talks talk\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation \n",
    "# the process of reducing inflected forms of a word white ensuring that the reduce form belongs to the language\n",
    "# the reduce form (root word) is called a lemma \n",
    "\n",
    "lemma_text = (\n",
    "    \"Gus is helping organise a developer\"\n",
    "    \" conference on Applications of Natural Language\"\n",
    "    \" Processing. He keeps organising local Python meetups\"\n",
    "    \" and several internal talks at his workplace.\"\n",
    ")\n",
    "\n",
    "lemma_doc = nlp(lemma_text)\n",
    "\n",
    "for token in lemma_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(token, token.lemma_)\n",
    "\n",
    "\n",
    "# this can be useful for the JobMatcher because if i have used a word in a different tense than the description .lemma_ will allow me to view the similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Word Frequency \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \" Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "words = [\n",
    "    token.text\n",
    "    for token in complete_doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "print(Counter(words).most_common(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Gus\n",
      "TAG: NNP\n",
      "POS: PROPN\n",
      "EXPLAINATION: noun, proper singular\n",
      "\n",
      "TOKEN: Proto\n",
      "TAG: NNP\n",
      "POS: PROPN\n",
      "EXPLAINATION: noun, proper singular\n",
      "\n",
      "TOKEN: is\n",
      "TAG: VBZ\n",
      "POS: AUX\n",
      "EXPLAINATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: a\n",
      "TAG: DT\n",
      "POS: DET\n",
      "EXPLAINATION: determiner\n",
      "\n",
      "TOKEN: Python\n",
      "TAG: NNP\n",
      "POS: PROPN\n",
      "EXPLAINATION: noun, proper singular\n",
      "\n",
      "TOKEN: developer\n",
      "TAG: NN\n",
      "POS: NOUN\n",
      "EXPLAINATION: noun, singular or mass\n",
      "\n",
      "TOKEN: currently\n",
      "TAG: RB\n",
      "POS: ADV\n",
      "EXPLAINATION: adverb\n",
      "\n",
      "TOKEN: working\n",
      "TAG: VBG\n",
      "POS: VERB\n",
      "EXPLAINATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: for\n",
      "TAG: IN\n",
      "POS: ADP\n",
      "EXPLAINATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: a\n",
      "TAG: DT\n",
      "POS: DET\n",
      "EXPLAINATION: determiner\n",
      "\n",
      "TOKEN: London\n",
      "TAG: NNP\n",
      "POS: PROPN\n",
      "EXPLAINATION: noun, proper singular\n",
      "\n",
      "TOKEN: -\n",
      "TAG: HYPH\n",
      "POS: PUNCT\n",
      "EXPLAINATION: punctuation mark, hyphen\n",
      "\n",
      "TOKEN: based\n",
      "TAG: VBN\n",
      "POS: VERB\n",
      "EXPLAINATION: verb, past participle\n",
      "\n",
      "TOKEN: Fintech\n",
      "TAG: NNP\n",
      "POS: PROPN\n",
      "EXPLAINATION: noun, proper singular\n",
      "\n",
      "TOKEN: company\n",
      "TAG: NN\n",
      "POS: NOUN\n",
      "EXPLAINATION: noun, singular or mass\n",
      "\n",
      "TOKEN: .\n",
      "TAG: .\n",
      "POS: PUNCT\n",
      "EXPLAINATION: punctuation mark, sentence closer\n",
      "\n",
      "TOKEN: He\n",
      "TAG: PRP\n",
      "POS: PRON\n",
      "EXPLAINATION: pronoun, personal\n",
      "\n",
      "TOKEN: is\n",
      "TAG: VBZ\n",
      "POS: AUX\n",
      "EXPLAINATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: interested\n",
      "TAG: JJ\n",
      "POS: ADJ\n",
      "EXPLAINATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "TOKEN: in\n",
      "TAG: IN\n",
      "POS: ADP\n",
      "EXPLAINATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: learning\n",
      "TAG: VBG\n",
      "POS: VERB\n",
      "EXPLAINATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: Natural\n",
      "TAG: NNP\n",
      "POS: PROPN\n",
      "EXPLAINATION: noun, proper singular\n",
      "\n",
      "TOKEN: Language\n",
      "TAG: NNP\n",
      "POS: PROPN\n",
      "EXPLAINATION: noun, proper singular\n",
      "\n",
      "TOKEN: Processing\n",
      "TAG: NNP\n",
      "POS: PROPN\n",
      "EXPLAINATION: noun, proper singular\n",
      "\n",
      "TOKEN: .\n",
      "TAG: .\n",
      "POS: PUNCT\n",
      "EXPLAINATION: punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# (PoS) Part-of-Speech Tagging \n",
    "# grammatical role that explains how a particular words is used in a sentance\n",
    "# Noun\n",
    "# Pronoun \n",
    "# Adjective\n",
    "# Verb\n",
    "# Adverb\n",
    "# Preposition\n",
    "# Conjunction\n",
    "# Interjection\n",
    "\n",
    "for token in about_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {str(token)}\n",
    "TAG: {str(token.tag_)}\n",
    "POS: {token.pos_}\n",
    "EXPLAINATION: {spacy.explain(token.tag_)}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns: [developer, company]\n",
      "adjectives: [interested]\n"
     ]
    }
   ],
   "source": [
    "# by using POS tags, you can extract a particular category of words\n",
    "\n",
    "nouns = []\n",
    "adjectives = []\n",
    "\n",
    "for token in about_doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjectives.append(token)\n",
    "\n",
    "print(f\"nouns: {nouns}\")\n",
    "print(f\"adjectives: {adjectives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"43252b8b1695409a83657398a9ced49b-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Tom</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">degree</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">statistical</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">mathematics.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43252b8b1695409a83657398a9ced49b-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43252b8b1695409a83657398a9ced49b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43252b8b1695409a83657398a9ced49b-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43252b8b1695409a83657398a9ced49b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43252b8b1695409a83657398a9ced49b-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43252b8b1695409a83657398a9ced49b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43252b8b1695409a83657398a9ced49b-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43252b8b1695409a83657398a9ced49b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43252b8b1695409a83657398a9ced49b-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43252b8b1695409a83657398a9ced49b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43252b8b1695409a83657398a9ced49b-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43252b8b1695409a83657398a9ced49b-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation using displaCy\n",
    "# you can use this to visualise dependency parse or named entities\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "about_interested_text = (\n",
    "    \"Tom has a degree in statistical mathematics.\"\n",
    ")\n",
    "\n",
    "about_interested_doc = nlp(about_interested_text)\n",
    "\n",
    "displacy.render(about_interested_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gus',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'developer',\n",
       " 'currently',\n",
       " 'work',\n",
       " 'london',\n",
       " 'base',\n",
       " 'fintech',\n",
       " 'company',\n",
       " 'interested',\n",
       " 'learn',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'developer',\n",
       " 'conference',\n",
       " 'happen',\n",
       " '21',\n",
       " 'july',\n",
       " '2019',\n",
       " 'london',\n",
       " 'title',\n",
       " 'application',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'helpline',\n",
       " 'number',\n",
       " 'available',\n",
       " '+44',\n",
       " '1234567891',\n",
       " 'gus',\n",
       " 'helping',\n",
       " 'organize',\n",
       " 'keep',\n",
       " 'organize',\n",
       " 'local',\n",
       " 'python',\n",
       " 'meetup',\n",
       " 'internal',\n",
       " 'talk',\n",
       " 'workplace',\n",
       " 'gus',\n",
       " 'present',\n",
       " 'talk',\n",
       " 'talk',\n",
       " 'introduce',\n",
       " 'reader',\n",
       " 'use',\n",
       " 'case',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'fintech',\n",
       " 'apart',\n",
       " 'work',\n",
       " 'passionate',\n",
       " 'music',\n",
       " 'gus',\n",
       " 'learn',\n",
       " 'play',\n",
       " 'piano',\n",
       " 'enrol',\n",
       " 'weekend',\n",
       " 'batch',\n",
       " 'great',\n",
       " 'piano',\n",
       " 'academy',\n",
       " 'great',\n",
       " 'piano',\n",
       " 'academy',\n",
       " 'situate',\n",
       " 'mayfair',\n",
       " 'city',\n",
       " 'london',\n",
       " 'world',\n",
       " 'class',\n",
       " 'piano',\n",
       " 'instructor']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing Functions \n",
    "# bringing text into a format ideal for analysis\n",
    "# 1. lowercase text\n",
    "# 2. lemmatise each token\n",
    "# 3. remove punctuation\n",
    "# 4. remove stop words\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "def is_token_allowed(token):\n",
    "    return bool(\n",
    "        token \n",
    "        and str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "    )\n",
    "\n",
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "complete_filtered_tokens = [\n",
    "    preprocess_token(token)\n",
    "    for token in complete_doc\n",
    "    if is_token_allowed(token)\n",
    "]\n",
    "\n",
    "complete_filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas Sharples\n"
     ]
    }
   ],
   "source": [
    "# Rule based matching \n",
    "# one of the steps in extracting information from unstructed text\n",
    "# to identidy and extract tokens and phrases according to patterns and grammatical features\n",
    "# you can use regex but rule based matching in spaCy is more powerful\n",
    "\n",
    "# Extracting first and last names\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "\n",
    "# names are always proper nouns so the pattern will be two consecutive proper nouns\n",
    "def extract_full_name(nlp_doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
    "    matcher.add(\"FULL NAME\", [pattern])\n",
    "    matches = matcher(nlp_doc)\n",
    "    for _, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "# test\n",
    "text = \"Hello my name is Thomas Sharples\"\n",
    "doc = nlp(text)\n",
    "print(extract_full_name(doc)) # and it bloody works mate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+447591216386\n"
     ]
    }
   ],
   "source": [
    "# Phone number extraction \n",
    "phone_text = \"Hello my name is Thomas Sharples, please contact me on +447591216386\"\n",
    "phone_doc = nlp(phone_text)\n",
    "\n",
    "def extract_phone_number(nlp_doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{\"TEXT\": {\"REGEX\": r\"(\\+447\\d{9}|07\\d{9})\"}}]\n",
    "    matcher.add(\"PHONE_NUMBER\", [pattern])\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "print(extract_phone_number(phone_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsharples101@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# email extraction\n",
    "def extract_email(nlp_doc):\n",
    "    for token in nlp_doc:\n",
    "        if token.like_email is True:\n",
    "            return token.text\n",
    "\n",
    "email_text = \"Hello my email is tsharples101@gmail.com\"\n",
    "email_doc = nlp(email_text)\n",
    "print(extract_email(email_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github.com/tomsharples1\n"
     ]
    }
   ],
   "source": [
    "# extracting url\n",
    "def extract_url(nlp_doc):\n",
    "    for token in nlp_doc:\n",
    "        if token.like_url is True:\n",
    "            return token.text\n",
    "\n",
    "url_text = \"Hello my github is github.com/tomsharples1\"\n",
    "url_doc = nlp(url_text)\n",
    "print(extract_url(url_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aleeya Lone', '07904536514', 'aleeya.lone@gmail.com', None]\n"
     ]
    }
   ],
   "source": [
    "def extract_all(nlp_doc):\n",
    "    info = []\n",
    "    info.append(extract_full_name(nlp_doc))\n",
    "    info.append(extract_phone_number(nlp_doc))      \n",
    "    info.append(extract_email(nlp_doc))\n",
    "    info.append(extract_url(nlp_doc))\n",
    "    return info\n",
    "\n",
    "long_text = \"hello my name is Aleeya Lone, my email is aleeya.lone@gmail.com and my number is 07904536514\"\n",
    "long_doc = nlp(long_text)\n",
    "\n",
    "print(extract_all(long_doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Tom\n",
      "token.tag_ ='NNP'\n",
      "token.head.text ='learning'\n",
      "token.dep_ ='nsubj'\n",
      "explaination = noun, proper singular\n",
      "\n",
      "TOKEN: is\n",
      "token.tag_ ='VBZ'\n",
      "token.head.text ='learning'\n",
      "token.dep_ ='aux'\n",
      "explaination = verb, 3rd person singular present\n",
      "\n",
      "TOKEN: learning\n",
      "token.tag_ ='VBG'\n",
      "token.head.text ='learning'\n",
      "token.dep_ ='ROOT'\n",
      "explaination = verb, gerund or present participle\n",
      "\n",
      "TOKEN: the\n",
      "token.tag_ ='DT'\n",
      "token.head.text ='piano'\n",
      "token.dep_ ='det'\n",
      "explaination = determiner\n",
      "\n",
      "TOKEN: piano\n",
      "token.tag_ ='NN'\n",
      "token.head.text ='learning'\n",
      "token.dep_ ='dobj'\n",
      "explaination = noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "# dependancy parsing \n",
    "# extracting a dependency graph of the sentance to represent its grammatical structure\n",
    "# it defines dependancy relationships between headwords and their dependents\n",
    "\n",
    "# head of a sentance has no dependency and is called the root of the sentance\n",
    "# the verb is usually the root of the sentance\n",
    "\n",
    "# Dependencies can be mapped in a graph where,\n",
    "# words are the nodes \n",
    "# grammatical relations are the edges\n",
    "# ayyy graph theory \n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "piano_text = \"Tom is learning the piano\"\n",
    "piano_doc = nlp(piano_text)\n",
    "\n",
    "for token in piano_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {token.text}\n",
    "{token.tag_ =}\n",
    "{token.head.text =}\n",
    "{token.dep_ =}\n",
    "explaination = {spacy.explain(token.tag_)}\"\"\"\n",
    "\n",
    "    )\n",
    "\n",
    "# nsubj is the subkect of the word\n",
    "# aux id sn auxiliary wod and its headword is a verb\n",
    "# dobj is the direct object of the verb, headword is also a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1270079343.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[240], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    displacy.render(piano_doc, style =)\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "displacy.render(piano_doc, style =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'Python', 'working']\n"
     ]
    }
   ],
   "source": [
    "# Tree and Subtree Navigation\n",
    "# the dependency graph has all the properties of a tree \n",
    "# this tree contains information about sentance structure and grammar.\n",
    "\n",
    "# spaCy provides attribues like .children, .lefts, .rights and .subtree to make navigating the parse tree easier.\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "one_line_about_text = (\n",
    "    \"Tom Sharples is a Python developer\"\n",
    "    \" currently working for a london-based fintech company\"\n",
    ")\n",
    "\n",
    "one_line_about_doc = nlp(one_line_about_text)\n",
    "\n",
    "# extrat children of 'developer'\n",
    "print([token.text for token in one_line_about_doc[5].children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n"
     ]
    }
   ],
   "source": [
    "# extract previous neighboring node of 'developer'\n",
    "print(one_line_about_doc[5].nbor(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently\n"
     ]
    }
   ],
   "source": [
    "# extract next neigboring node of 'developer'\n",
    "print(one_line_about_doc[5].nbor())\n",
    "\n",
    "# remebering that the words are the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'Python']\n",
      "['working']\n"
     ]
    }
   ],
   "source": [
    "# extracting all the tokens on the right of 'developer'\n",
    "print([token.text for token in one_line_about_doc[5].lefts])\n",
    "print([token.text for token in one_line_about_doc[5].rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a, Python, developer, currently, working, for, a, london, -, based, fintech, company]\n"
     ]
    }
   ],
   "source": [
    "# Print subtree of 'developer'\n",
    "\n",
    "print(list(one_line_about_doc[5].subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a developer conference\n",
      "21 July\n",
      "London\n"
     ]
    }
   ],
   "source": [
    "# Shallow Parsing / chunking\n",
    "# the process of extracting phrases from unstructured text\n",
    "# chunking groups of adjacent tokens in phrases on the basis of their PoS tags.\n",
    "\n",
    "# Noun Phrase detection \n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "conference_text = (\n",
    "    \"There is a developer conference happening on 21 July 2019 in London\"\n",
    ")\n",
    "\n",
    "conference_doc = nlp(conference_text)\n",
    "\n",
    "# extract noun phrases\n",
    "\n",
    "for chunk in conference_doc.noun_chunks:\n",
    "    print(chunk)\n",
    "\n",
    "# this is a nice way to summarise key information from the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ent.text = 'Great Paino Academy'\n",
      "        ent.start_char = 0\n",
      "        ent.end_char = 19\n",
      "        ent.label_ = 'ORG'\n",
      "        label explaination = Companies, agencies, institutions, etc.\n",
      "\n",
      "        ent.text = 'Mayfair'\n",
      "        ent.start_char = 35\n",
      "        ent.end_char = 42\n",
      "        ent.label_ = 'GPE'\n",
      "        label explaination = Countries, cities, states\n",
      "\n",
      "        ent.text = 'the City of London'\n",
      "        ent.start_char = 46\n",
      "        ent.end_char = 64\n",
      "        ent.label_ = 'GPE'\n",
      "        label explaination = Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition \n",
    "# is the process of locating names entities in unstructured text\n",
    "# and then classifying them into predefined categories\n",
    "\n",
    "piano_class_text = (\n",
    "    \"Great Paino Academy is situated\"\n",
    "    \" in Mayfair or the City of London and has\"\n",
    "    \" world-class piano instructors\"\n",
    ")\n",
    "\n",
    "piano_class_doc = nlp(piano_class_text)\n",
    "\n",
    "for ent in piano_class_doc.ents:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        {ent.text = }\n",
    "        {ent.start_char = }\n",
    "        {ent.end_char = }\n",
    "        {ent.label_ = }\n",
    "        label explaination = {spacy.explain(ent.label_)}\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Paino Academy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is situated in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mayfair\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the City of London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and has world-class piano instructors</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(piano_class_doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5 people surveyed, REDACTED , REDACTED and REDACTED like apples. REDACTED and REDACTED like oranges.\n"
     ]
    }
   ],
   "source": [
    "# name redaction using NER \n",
    "\n",
    "survey_text = (\n",
    "    \"Out of 5 people surveyed, James Robert,\"\n",
    "    \" Julie Fuller and Benjamin Brooks like\"\n",
    "    \" apples. Kelly Cox and Matthew Evans\"\n",
    "    \" like oranges.\"\n",
    ")\n",
    "\n",
    "survey_doc = nlp(survey_text)\n",
    "\n",
    "def replace_name(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"REDACTED \"\n",
    "    return token.text_with_ws\n",
    "\n",
    "def redact_names(nlp_doc):\n",
    "    with nlp_doc.retokenize() as retokeniser:\n",
    "        for ent in nlp_doc.ents:\n",
    "            retokeniser.merge(ent)\n",
    "    tokens = map(replace_name, nlp_doc)\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "print(redact_names(survey_doc))\n",
    "\n",
    "def is_name(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume NLP parsing \n",
    "\n",
    "resume = \"Thomas Sharples    Tsharples101@gmail.com       https://github.com/tomsharples1      07591216386  Education   BSc (Hons) in Mathematics with Statistics - University of Nottingham, UK                                                         Grade: First Class (1st)  Final Year Group Project: 74%     Key Modules:   Applied Statistical Modelling: General Linear Models, Logistic Regression and Modelling with Survival Data, both theoretically and in R.  Multivariate Analysis: Handling high dimensional data and performing PCA, Linear Discriminant Analysis and Clustering, both theoretically and in R.    Statistical Inference: Maximum Likelihood Estimation, Bayesian Inference and Markov Chain Monte Carlo.     Monk’s Walk Sixth Form - Hertfordshire, UK                                                                         A-Levels  Further Mathematics (A*), Mathematics (A*), Chemistry (C)      Technical Skills   • Python: Proficient in data manipulation, supervised & unsupervised machine learning, using APIs and numerical methods. Experienced with Jupyter Notebook and libraries such as NumPy, Scikit-learn, Pandas, and SerpApi.  • R: Proficient in regression models, hypothesis testing and machine learning techniques such as PCA and clustering. Skilled in data manipulation, visualisation and analysis using the tidyverse suite.   • Data Visualisation: Expertise in visualising data using libraries such as Matplotlib in Python and ggplot2 and factoextra in R.  • Languages: Highly skilled in French speaking, reading and writing developed in Interfaculty French Level 3 and Level 4. Conversational proficiency in German handling day to day interactions well and a developing knowledge of Arabic.    Projects  Data Science Project – Grade: 100%  Analysed a United Nations data set to explore the economic and health indicators of various countries using machine learning and multivariate analysis techniques in R.  • PCA: Plotting and interpreting principal components to understand the similarities between countries by GDP and life expectancy.   • Clustering: Assessing whether k-means, model based or hierarchical clustering methods yielded the best results in classifying countries based on GDP and life expectancy.   • Linear Discriminant Analysis: Predicting continent based on GDP, life expectancy and population data and assessing the predictive accuracy using a confusion matrix.   • Regression: Building a linear model using OLS, principal component and ridge regression to predict life expectancy in 2007 and determine which method gave the most accurate model.  Job Matcher Machine Learning Algorithm  Developed a program in Python that matches job descriptions with CVs using machine learning, outputting a table of jobs with the highest percentage matches. Using Jupyter Notebook to provide real-time updates to the table as new data was streamed from the API.  • APIs: Developed my knowledge of APIs by using SerpApi to web scrape data from Google Jobs to input into my machine learning algorithm.   • Machine Learning: Improved machine learning skills by learning text similarity algorithms and developed my understanding of Scikit-learn library.  • Future plans include developing this project into an NLP based application to improve the accuracy of the percentage match.  Final Year Applied Mathematical Modelling Group Projects - Grade 74%  Project 1: Modelling Javelin Trajectories, Project 2: Modelling Bumper Car Dynamics, Project 3: Modelling Spontaneous Ignition.   • Python Skills: Applied object-oriented programming to solve differential equations using numerical methods such as the Runge-Kutta method and the Finite Difference Method.  • Machine Learning: Employing machine learning polynomial regression to predict the flight path in project one, using Sci-kit learn, NumPy and Matplotlib libraries.  • Mathematical Report Writing: Following a model, analysis, results (MAR) structure to show our findings clearly and effectively.  • Presentation: Presenting our project results to a large audience, ensuring complex ideas were understandable to various mathematical levels.    Experience   U-Cycle Business Development Officer and Operations Advisor - Enactus Nottingham, 2024  • 1000% increase in reach on Instagram by managing and developing relationships between societies and companies, contributing to a £1400 increase in revenue since the previous year.  • Refined leadership skills while organising a competition in collaboration with an engineering society, engaging 50 participants and hosting multiple events including a final judging panel event.  • Performed analysis on CO2 emissions while running a social media campaign with sustainability society to promote Ucycle’s environmental benefits.  • Advised operational decisions including supplier selection, pricing negotiation and cost analysis.    Great British Cycling Team Cyclist – UK, 2019 - 2021  • Transferred Athletic Discipline built by working towards challenging goals and understanding the importance of the long game to my academic studies.  • Balanced training, travel and education developing my time management skills.   • Analysed Real Data from training by identifying patterns, allowing me to make informed decisions around training progress.   • Performed at the Highest Level and Succeeded under Pressure, highlighted by a European Bronze medal in the 1km time trial in 2021\"\n",
    "\n",
    "cv_text = \"\"\"\n",
    "John Doe\n",
    "Email: johndoe@example.com\n",
    "Phone: 123-456-7890\n",
    "\n",
    "Experience:\n",
    "- Software Engineer at TechCorp (2018 - Present)\n",
    "  - Developed a customer relationship management system using Python and Django.\n",
    "  - Led a team of 5 developers.\n",
    "  - Improved system performance by 30%.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, Java, C++\n",
    "- Web Development: HTML, CSS, JavaScript, Django, Flask\n",
    "- Databases: MySQL, PostgreSQL\n",
    "- Tools: Git, Docker, Jenkins\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "def preprocessing(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = [token.lemma_.strip().lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = preprocessing(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "cv_text = preprocessing(cv_text)\n",
    "cv_doc = nlp(cv_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher \n",
    "from collections import OrderedDict\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_skills(nlp_doc):\n",
    "    pattern = [{\"LOWER\": {\"IN\": ['python', \"r\", \"data visualisation\", \n",
    "                                 \"sql\", \"scala\", \"julia\", \"pandas\", \"scikit learn\", \"numpy\", \"spacy\", \"nlp\",\n",
    "                                 \"llm\", 'tidyverse', 'dplyr', 'ggplot2', 'ggplot', 'clustering', 'pca', 'regression',\n",
    "                                 'logistic', 'statistical', 'statistics', 'bayesian', 'hypothesis testing', 'tensorflow',\n",
    "                                 'pyspark', 'databricks', 'pytorch', 'k','nearest','neighbour', 'knn', 'random','forest', 'means',\n",
    "                                 'dimensional','reduction', 'matplotlib', 'seaborn', 'powerbi', 'tableau', 'etl', 'elt', 'nosql',\n",
    "                                 'warehousing', 'warehouse', 'lake', 'mining', 'cleaning', 'wrangling',\n",
    "                                 'neural','network', 'chatbot', 'gpt', 'bert', 'ntlk','aws', 'gcp','azure', 'git', 'github', \n",
    "                                 'b testing', 'query', 'queries', 'api', 'apis', 'web', 'scraping', 'webscraping', 'mysql', \n",
    "                                 'machine', 'learn', 'learning' 'cloud', 'platform']}}]\n",
    "    matcher.add(\"SKILLS\", [pattern])\n",
    "    matches = matcher(nlp_doc)\n",
    "\n",
    "    skills = []\n",
    "    for _, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        skills.append(span.text)\n",
    "    \n",
    "    # removing duplicates\n",
    "    skills = list(OrderedDict.fromkeys(skills))\n",
    "\n",
    "    return skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['statistics', 'statistical', 'logistic', 'regression', 'dimensional', 'pca', 'clustering', 'bayesian', 'python', 'machine', 'api', 'numpy', 'learn', 'pandas', 'r', 'tidyverse', 'matplotlib', 'ggplot2', 'k', 'web', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "resume_doc = nlp(resume)\n",
    "\n",
    "print(extract_skills(resume_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

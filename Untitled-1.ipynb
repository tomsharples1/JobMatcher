{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"5d6289c06d2b6cfce9db2909be346b6b981f62123c9670809947981ec7c65ed9\"\n",
    "\n",
    "resume = \"Thomas Sharples   Email: tsharples101@gmail.com   Mobile: 07591216386  Education   Mathematics BSc University of Nottingham – UK                                                          Sept 2021-July 2024    Current Grade: First    Key Modules:   Applied Statistical Modelling: The study of General Linear Models, link functions (logit) and modelling with survival data. Theoretically and in R.   Multivariate Analysis: Handling high dimensional data and performing PCA, Linear Discriminant Analysis and Clustering theoretically and in R.    Statistical Inference: Applying Maximum Likelihood Estimation, Bayesian Inference and Markov Chain Monte Carlo to draw insights and make informed decisions from data.   Exam grade (89%)      Monk’s Walk Sixth Form – Hertfordshire, UK                                                                       Sept 2019-May 2021  A-Levels                                                                                                             Subjects: Further Mathematics (A*), Mathematics (A*), Chemistry (C)      Technical Skills   • Proficient in Python with expertise in numerical methods, web scraping using APIs and data frame manipulation, utilising Jupyter Notebook and libraries such as NumPy, Matplotlib, Sci-kit learn, SerpApi and Pandas.  • Experienced in R programming using linear regression, hypothesis testing, performing PCA, clustering and data visualisation using libraries such as ggplot2, dplyr and factoextra.  • Highly skilled in French speaking, reading and writing developed in Interfaculty French Level 3 and Level 4.     Projects  Watts2speed App - 2022  Working with GB podium cyclists to develop an app that enables them to determine their speed based on their power output.   • Combined cycling and mathematical knowledge to build an initial app in Python.  • Rapidly adapted to C#, Xaml, and .NET Maui after realising the mobile interface limitations of Python.  • Tested using real data provided by my friend Josh Charlton’s to verify the output’s accuracy.  • Resulted in an app that opened discussions on watt savings and their corresponding speed gains. Enabled precise pacing calculations and required power outputs for specific target times. This placed Josh 2nd at the 2022 European Championships for Individual Pursuit and 6th at the 2023 World Time Trial Championships.  Applied Mathematical Modelling Group Projects – 2024  Project 1: Modelling Javelin Trajectories, Project 2: Modelling Bumper Car Dynamics, Project 3: Modelling Spontaneous Ignition.   • Proficiently developed Python skills and applied object-oriented programming to solve differential equations using numerical methods such as the Runge-Kutta method and the Finite Difference Method.  • Employed machine learning polynomial regression for flight path prediction in project one, using Sci-kit learn, NumPy and Matplotlib libraries.  • Mathematical report writing following a model, analysis, results (MAR) structure to show our findings clearly and effectively.  • Presented project results to a large audience, ensuring complex ideas were understandable to various mathematical levels.      Experience   U-Cycle Business Development Officer - Enactus Nottingham – 2024  • 1000% increase in reach on Instagram by managing and developing relationships between societies and companies, contributing to a £1400 increase in revenue since the previous year.  • Refined leadership skills while organising a competition in collaboration with an engineering society, engaging 50 participants and hosting multiple events including a final judging panel event.  • Performed analysis on CO2 emissions while running a social media campaign with sustainability society to promote Ucycle’s environmental benefits.  • Advised operational decisions including supplier selection, pricing negotiation and cost analysis.    Great British Cycling Team Cyclist – UK – 2019-2021  • Transferred athletic discipline built by working towards challenging goals and understanding the importance of the long game to my academic studies.  • Balanced training, travel and education developing my time management skills.   • Analysed real data from training by identifying patterns, allowing me to make informed decisions around training progress.   • Performed at the highest level and succeeded under pressure, highlighted by a European Bronze medal in the 1km time trial in 2021.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = {\"Data scientist\"} # change this later \n",
    "search_location = \"London, United Kingdom\"\n",
    "\n",
    "params = {\n",
    "    \"q\": search_term,\n",
    "    \"location\": search_location,\n",
    "    \"engine\": \"google_jobs\",\n",
    "    \"hl\": 'en',\n",
    "    \"gl\": \"uk\",\n",
    "    \"api_key\": api_key,\n",
    "    \"chips\": \"date_posted:today\",\n",
    "    \"start\": 0,\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search.get_dict()\n",
    "jobs_df = results[\"jobs_results\"]\n",
    "jobs_df = pd.DataFrame(jobs_df)\n",
    "#jobs_df = pd.concat([pd.DataFrame(jobs_df), pd.json_normalize(jobs_df[\"detected_extensions\"])], axis=1).drop('detected_extensions')\n",
    "\n",
    "jobs_df = jobs_df.iloc[:, [0, 1,2,3,4,5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>job_highlights</th>\n",
       "      <th>Match %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chief Data Scientist</td>\n",
       "      <td>myGwork - LGBTQ+ Business Community</td>\n",
       "      <td>London</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>This inclusive employer is a member of myGwork...</td>\n",
       "      <td>[{'items': ['This inclusive employer is a memb...</td>\n",
       "      <td>71.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Game Integrity &amp; Fraud</td>\n",
       "      <td>Scopely</td>\n",
       "      <td>London</td>\n",
       "      <td>via Games Jobs Direct</td>\n",
       "      <td>Description\\n\\nScopely is looking for a Data S...</td>\n",
       "      <td>[{'items': ['Description\n",
       "\n",
       "Scopely is looking f...</td>\n",
       "      <td>70.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist II, SEM</td>\n",
       "      <td>Expedia Group</td>\n",
       "      <td>London</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>If you need assistance during the recruiting p...</td>\n",
       "      <td>[{'items': ['If you need assistance during the...</td>\n",
       "      <td>70.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist, GenAI Innovation Center</td>\n",
       "      <td>Amazon Web Services (AWS)</td>\n",
       "      <td>London</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Description\\n\\nAre you looking to work at the ...</td>\n",
       "      <td>[{'items': ['Description\n",
       "\n",
       "Are you looking to w...</td>\n",
       "      <td>69.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist, Decision Science and Te...</td>\n",
       "      <td>Amazon TA</td>\n",
       "      <td>London</td>\n",
       "      <td>via WAVY Jobs</td>\n",
       "      <td>Are you inspired by unique and challenging pro...</td>\n",
       "      <td>[{'items': ['Are you inspired by unique and ch...</td>\n",
       "      <td>69.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>People Insights &amp; Analytics Lead Data Scientist</td>\n",
       "      <td>bp</td>\n",
       "      <td>Sunbury-on-Thames</td>\n",
       "      <td>via Indeed</td>\n",
       "      <td>Job summary\\n\\nEntity:\\nPeople &amp; Culture\\n\\nJo...</td>\n",
       "      <td>[{'items': ['Job summary\n",
       "\n",
       "Entity:\n",
       "People &amp; Cul...</td>\n",
       "      <td>67.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist, Decision Science and Te...</td>\n",
       "      <td>Amazon TA</td>\n",
       "      <td>London</td>\n",
       "      <td>via WAVY Jobs</td>\n",
       "      <td>Are you inspired by unique and challenging pro...</td>\n",
       "      <td>[{'items': ['Are you inspired by unique and ch...</td>\n",
       "      <td>66.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Understanding Recruitment</td>\n",
       "      <td>Greater London</td>\n",
       "      <td>via Recruit.net</td>\n",
       "      <td>Senior Data Scientist | NLP &amp; LLMs\\n\\nWould yo...</td>\n",
       "      <td>[{'items': ['Senior Data Scientist | NLP &amp; LLM...</td>\n",
       "      <td>57.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Contract Data Scientist / ML Specialist (Compu...</td>\n",
       "      <td>Chameleon IT Solutions Limited</td>\n",
       "      <td>London</td>\n",
       "      <td>via reed.co.uk</td>\n",
       "      <td>Contract Data Scientist / Machine Leaning spec...</td>\n",
       "      <td>[{'items': ['Contract Data Scientist / Machine...</td>\n",
       "      <td>45.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - up to £80k + Bonus - Hybrid/L...</td>\n",
       "      <td>Hunter Bond</td>\n",
       "      <td>London</td>\n",
       "      <td>via CRA Group</td>\n",
       "      <td>Job Title: Data Scientist\\n\\nClient: Fast Scal...</td>\n",
       "      <td>[{'items': ['Job Title: Data Scientist\n",
       "\n",
       "Client...</td>\n",
       "      <td>30.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                               Chief Data Scientist   \n",
       "1            Data Scientist - Game Integrity & Fraud   \n",
       "2                             Data Scientist II, SEM   \n",
       "3            Data Scientist, GenAI Innovation Center   \n",
       "4  Senior Data Scientist, Decision Science and Te...   \n",
       "5    People Insights & Analytics Lead Data Scientist   \n",
       "6  Senior Data Scientist, Decision Science and Te...   \n",
       "7                              Senior Data Scientist   \n",
       "8  Contract Data Scientist / ML Specialist (Compu...   \n",
       "9  Data Scientist - up to £80k + Bonus - Hybrid/L...   \n",
       "\n",
       "                          company_name                location  \\\n",
       "0  myGwork - LGBTQ+ Business Community               London      \n",
       "1                              Scopely               London      \n",
       "2                        Expedia Group               London      \n",
       "3            Amazon Web Services (AWS)               London      \n",
       "4                            Amazon TA               London      \n",
       "5                                   bp    Sunbury-on-Thames      \n",
       "6                            Amazon TA               London      \n",
       "7            Understanding Recruitment       Greater London      \n",
       "8       Chameleon IT Solutions Limited               London      \n",
       "9                          Hunter Bond               London      \n",
       "\n",
       "                     via                                        description  \\\n",
       "0           via LinkedIn  This inclusive employer is a member of myGwork...   \n",
       "1  via Games Jobs Direct  Description\\n\\nScopely is looking for a Data S...   \n",
       "2           via LinkedIn  If you need assistance during the recruiting p...   \n",
       "3           via LinkedIn  Description\\n\\nAre you looking to work at the ...   \n",
       "4          via WAVY Jobs  Are you inspired by unique and challenging pro...   \n",
       "5             via Indeed  Job summary\\n\\nEntity:\\nPeople & Culture\\n\\nJo...   \n",
       "6          via WAVY Jobs  Are you inspired by unique and challenging pro...   \n",
       "7        via Recruit.net  Senior Data Scientist | NLP & LLMs\\n\\nWould yo...   \n",
       "8         via reed.co.uk  Contract Data Scientist / Machine Leaning spec...   \n",
       "9          via CRA Group  Job Title: Data Scientist\\n\\nClient: Fast Scal...   \n",
       "\n",
       "                                      job_highlights  Match %  \n",
       "0  [{'items': ['This inclusive employer is a memb...    71.43  \n",
       "1  [{'items': ['Description\n",
       "\n",
       "Scopely is looking f...    70.65  \n",
       "2  [{'items': ['If you need assistance during the...    70.26  \n",
       "3  [{'items': ['Description\n",
       "\n",
       "Are you looking to w...    69.47  \n",
       "4  [{'items': ['Are you inspired by unique and ch...    69.00  \n",
       "5  [{'items': ['Job summary\n",
       "\n",
       "Entity:\n",
       "People & Cul...    67.20  \n",
       "6  [{'items': ['Are you inspired by unique and ch...    66.07  \n",
       "7  [{'items': ['Senior Data Scientist | NLP & LLM...    57.89  \n",
       "8  [{'items': ['Contract Data Scientist / Machine...    45.29  \n",
       "9  [{'items': ['Job Title: Data Scientist\n",
       "\n",
       "Client...    30.26  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "descriptions = jobs_df[\"description\"]\n",
    "\n",
    "def compare(CV, job_desc):\n",
    "    compare = [CV, job_desc]\n",
    "    cVect = CountVectorizer()\n",
    "    cMatrix = cVect.fit_transform(compare)\n",
    "\n",
    "    matPerecent = cosine_similarity(cMatrix)[0][1] * 100 \n",
    "    matPerecent = round(matPerecent, 2)\n",
    "\n",
    "    return matPerecent\n",
    "\n",
    "match_percentage =[]\n",
    "for i in range(len(jobs_df)):\n",
    "    match_percentage.append(compare(resume, descriptions.iloc[i]))\n",
    "\n",
    "jobs_df[\"Match %\"] = match_percentage\n",
    "\n",
    "jobs_df.sort_values(by=\"Match %\", ascending=False, inplace=True)\n",
    "\n",
    "jobs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "jobs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m get_stop_words(descriptions[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     25\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m word_count_vector \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mlist\u001b[39m(cv\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfTransformer\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1398\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1396\u001b[0m min_doc_count \u001b[38;5;241m=\u001b[39m min_df \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(min_df, Integral) \u001b[38;5;28;01melse\u001b[39;00m min_df \u001b[38;5;241m*\u001b[39m n_doc\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_doc_count \u001b[38;5;241m<\u001b[39m min_doc_count:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_df corresponds to < documents than min_df\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1400\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "\u001b[0;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "# using sklearn to find keywrods in text \n",
    "import re \n",
    "\n",
    "def pre_process(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", text)\n",
    "\n",
    "    return text \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_stop_words(stop_words_str):\n",
    "    stopwords = stop_words_str.split() # reads the lines in the file \n",
    "    stop_set = set(word.strip() for word in stopwords) # strip() method to remove any leading and tailing whitespac\n",
    "    return frozenset(stop_set)\n",
    "    \n",
    "# frozenset means that when you fix a set of elements it cant change\n",
    "\n",
    "doc = descriptions[0]\n",
    "\n",
    "stopwords = get_stop_words(descriptions[0])\n",
    "cv = CountVectorizer(max_df=0.85, stop_words=\"english\")\n",
    "word_count_vector = cv.fit_transform([doc])\n",
    "\n",
    "list(cv.vocabulary_.keys())[:10]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "tf_ind_vec = tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip (coo_matrix.col, coo_matrix. data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "                              \n",
    "def extract_top_from_vector(feature_names, sorted_items, top=10):\n",
    "    sorted_items = sorted_items [: top]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "# word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        score_vals.append(round(score, 3)) #keep track of feature name and its corresponding score score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "sorted_items = sort_coo(tf_ind_vec.tocoo())\n",
    "\n",
    "keywords= extract_top_from_vector(feature_names, sorted_items, 20)\n",
    "\n",
    "for k in keywords:\n",
    "    print(k, keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 92)\t2\n",
      "  (0, 524)\t6\n",
      "  (0, 274)\t1\n",
      "  (0, 121)\t3\n",
      "  (0, 509)\t1\n",
      "  (0, 181)\t2\n",
      "  (0, 333)\t1\n",
      "  (0, 153)\t1\n",
      "  (0, 340)\t1\n",
      "  (0, 525)\t1\n",
      "  (0, 439)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 511)\t3\n",
      "  (0, 477)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 239)\t1\n",
      "  (0, 134)\t1\n",
      "  (0, 433)\t2\n",
      "  (0, 304)\t1\n",
      "  (0, 505)\t3\n",
      "  (0, 215)\t5\n",
      "  (0, 185)\t1\n",
      "  (0, 120)\t1\n",
      "  (0, 87)\t2\n",
      "  (0, 54)\t1\n",
      "  :\t:\n",
      "  (9, 500)\t1\n",
      "  (9, 463)\t3\n",
      "  (9, 468)\t1\n",
      "  (9, 295)\t1\n",
      "  (9, 278)\t1\n",
      "  (9, 302)\t1\n",
      "  (9, 428)\t1\n",
      "  (9, 492)\t1\n",
      "  (9, 490)\t1\n",
      "  (9, 219)\t1\n",
      "  (9, 324)\t1\n",
      "  (9, 348)\t1\n",
      "  (9, 190)\t2\n",
      "  (9, 369)\t1\n",
      "  (9, 420)\t1\n",
      "  (9, 252)\t1\n",
      "  (9, 29)\t1\n",
      "  (9, 407)\t2\n",
      "  (9, 414)\t1\n",
      "  (9, 73)\t1\n",
      "  (9, 496)\t1\n",
      "  (9, 201)\t1\n",
      "  (9, 161)\t1\n",
      "  (9, 115)\t1\n",
      "  (9, 255)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import pandas as pd \n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\", max_df=0.9, min_df=2)\n",
    "word_counts = cv.fit_transform(jobs_df[\"description\"])\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000' '2023' '25' 'abilities' 'ability' 'able' 'academic' 'access'\n",
      " 'accessible' 'accountabilities' 'achieve' 'achievements' 'active'\n",
      " 'activities' 'add' 'advanced' 'agile' 'ai' 'algorithms' 'alongside'\n",
      " 'analysis' 'analysts' 'analytical' 'analytics' 'annual' 'applicants'\n",
      " 'application' 'applications' 'applied' 'apply' 'applying' 'approach'\n",
      " 'approaches' 'areas' 'array' 'art' 'asset' 'assets' 'attention'\n",
      " 'audience' 'authentic' 'automate' 'available' 'aws' 'bachelor' 'balance'\n",
      " 'bank' 'based' 'bases' 'basic' 'believe' 'benefits' 'best' 'better'\n",
      " 'billion' 'brand' 'bring' 'build' 'building' 'business' 'businesses'\n",
      " 'buy' 'candidates' 'capabilities' 'carbon' 'career' 'celebrate' 'chain'\n",
      " 'challenging' 'change' 'characteristics' 'choice' 'city' 'classification'\n",
      " 'clean' 'clear' 'clients' 'closely' 'cloud' 'clustering' 'code' 'coding'\n",
      " 'collaborate' 'collaboration' 'collaborative' 'come' 'commitment'\n",
      " 'committed' 'communication' 'communities' 'community' 'companies'\n",
      " 'company' 'competency' 'competitive' 'completely' 'complex'\n",
      " 'comprehensive' 'computer' 'concepts' 'connect' 'considered' 'context'\n",
      " 'continuous' 'contribute' 'control' 'craft' 'create' 'creating'\n",
      " 'creation' 'cross' 'culture' 'customer' 'customers' 'cutting' 'cv'\n",
      " 'dashboards' 'databases' 'databricks' 'datasets' 'date' 'day' 'days'\n",
      " 'decision' 'decisions' 'deep' 'define' 'degree' 'deliver' 'delivered'\n",
      " 'delivering' 'delivery' 'demonstrable' 'depending' 'deploying'\n",
      " 'deployment' 'design' 'designed' 'designing' 'desire' 'develop'\n",
      " 'developing' 'development' 'differences' 'diverse' 'diversity' 'domain'\n",
      " 'driven' 'driving' 'ds' 'dynamic' 'edge' 'effective' 'efficiently'\n",
      " 'employee' 'employees' 'employer' 'employing' 'encourage' 'end'\n",
      " 'engagement' 'engaging' 'engineering' 'english' 'enjoy' 'ensure'\n",
      " 'ensuring' 'enterprise' 'enthusiastic' 'environment' 'equal' 'equitable'\n",
      " 'equity' 'equivalent' 'essential' 'etl' 'excellent' 'existing'\n",
      " 'experienced' 'experiences' 'experimental' 'expertise' 'experts'\n",
      " 'explain' 'exploratory' 'facilitate' 'fast' 'faster' 'feedback' 'field'\n",
      " 'financial' 'fixed' 'flexibility' 'flexible' 'focus' 'following'\n",
      " 'forecasting' 'functional' 'future' 'gender' 'general' 'generate'\n",
      " 'genuine' 'git' 'global' 'goals' 'good' 'government' 'graph' 'group'\n",
      " 'growing' 'hand' 'hard' 'health' 'heart' 'help' 'helping' 'high' 'highly'\n",
      " 'hiring' 'holidays' 'home' 'house' 'hub' 'hybrid' 'ideas' 'identify'\n",
      " 'images' 'impact' 'implement' 'implementation' 'implementing'\n",
      " 'importance' 'important' 'improvement' 'improvements' 'improving'\n",
      " 'include' 'includes' 'including' 'inclusion' 'inclusive' 'income'\n",
      " 'independently' 'individual' 'industry' 'information' 'infrastructure'\n",
      " 'initiative' 'innovative' 'insights' 'inspired' 'instruments'\n",
      " 'integration' 'integrity' 'intellectual' 'intelligence' 'interested'\n",
      " 'intern' 'internal' 'interpersonal' 'issues' 'job' 'join' 'joining'\n",
      " 'journey' 'just' 'keen' 'key' 'know' 'knowledge' 'language' 'languages'\n",
      " 'large' 'largest' 'lead' 'leader' 'leaders' 'leading' 'learn' 'learning'\n",
      " 'leave' 'level' 'levels' 'leverage' 'life' 'like' 'line' 'live' 'lives'\n",
      " 'll' 'location' 'london' 'long' 'looking' 'love' 'machine' 'maintain'\n",
      " 'maintenance' 'make' 'making' 'management' 'managers' 'manner' 'market'\n",
      " 'markets' 'mathematical' 'mathematics' 'matter' 'meaningful' 'means'\n",
      " 'media' 'medical' 'members' 'methodologies' 'methods' 'metrics' 'minimum'\n",
      " 'ml' 'mlops' 'model' 'modelling' 'models' 'moment' 'monitoring'\n",
      " 'motivated' 'multi' 'multiple' 'nation' 'natural' 'need' 'needed' 'needs'\n",
      " 'networks' 'neural' 'new' 'nlp' 'non' 'offer' 'office' 'offices' 'online'\n",
      " 'open' 'operate' 'operations' 'opportunities' 'opportunity' 'optimize'\n",
      " 'options' 'order' 'outcomes' 'package' 'particularly' 'partner' 'passion'\n",
      " 'passionate' 'people' 'perform' 'performance' 'personal' 'physics'\n",
      " 'pipelines' 'place' 'platform' 'play' 'player' 'plus' 'points' 'policies'\n",
      " 'portfolio' 'positioned' 'possible' 'potential' 'power' 'practical'\n",
      " 'practice' 'practices' 'preferred' 'previous' 'privacy' 'private'\n",
      " 'proactively' 'problem' 'problems' 'process' 'processes' 'processing'\n",
      " 'product' 'production' 'products' 'professional' 'proficiency' 'program'\n",
      " 'programme' 'programs' 'progression' 'project' 'projects' 'protection'\n",
      " 'proud' 'proven' 'provide' 'providing' 'purpose' 'pytorch'\n",
      " 'qualifications' 'quality' 'quantitative' 'range' 'rationale' 'reach'\n",
      " 'real' 'recognition' 'reduce' 'regression' 'regular' 'relationships'\n",
      " 'relevant' 'reliability' 'rely' 'required' 'requirements' 'research'\n",
      " 'researchers' 'resources' 'respective' 'responsibilities' 'results'\n",
      " 'retrieval' 'reward' 'right' 'role' 'salary' 'scale' 'scheme' 'science'\n",
      " 'scientist' 'scientists' 'security' 'seek' 'seeking' 'self' 'senior'\n",
      " 'series' 'service' 'services' 'set' 'sets' 'shape' 'share' 'shared'\n",
      " 'similar' 'skills' 'social' 'societal' 'software' 'solutions' 'solve'\n",
      " 'solving' 'source' 'space' 'special' 'specific' 'sql' 'stakeholders'\n",
      " 'standards' 'start' 'state' 'statistical' 'statistics' 'strategy'\n",
      " 'street' 'strong' 'success' 'successful' 'successfully' 'summary'\n",
      " 'supply' 'support' 'supportive' 'sustainability' 'systems' 'tableau'\n",
      " 'tackle' 'talent' 'talented' 'targets' 'teams' 'technical' 'technically'\n",
      " 'techniques' 'technologies' 'technology' 'term' 'terms' 'testing' 'tests'\n",
      " 'thrive' 'tickets' 'time' 'tools' 'training' 'tuning' 'uk' 'understand'\n",
      " 'understanding' 'unique' 'uniquely' 'units' 'use' 'using' 'value'\n",
      " 'values' 'various' 'version' 'visualization' 'visualizations' 'voice'\n",
      " 'way' 'wide' 'wider' 'workforce' 'workplace' 'works' 'world' 'worldwide'\n",
      " 'writing' 'written' 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "features = cv.get_feature_names_out()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 288, 0.32194805624988526), (0, 488, 0.20445349469536886), (0, 464, 0.18086814629995665), (0, 499, 0.1468886361052171), (0, 411, 0.13797773839280797), (0, 356, 0.12521405262398613), (0, 332, 0.12267209681722133), (0, 478, 0.1205787641999711), (0, 421, 0.11016647707891283), (0, 279, 0.11016647707891283), (0, 250, 0.11016647707891283), (0, 87, 0.11016647707891283), (0, 452, 0.10980695435335693), (0, 475, 0.10514009488116163), (0, 419, 0.10514009488116163), (0, 379, 0.10514009488116163), (0, 274, 0.10514009488116163), (0, 258, 0.10514009488116163), (0, 135, 0.10514009488116163), (0, 524, 0.09959312624635043), (0, 162, 0.09959312624635043), (0, 438, 0.09198515892853865), (0, 355, 0.09198515892853865), (0, 247, 0.09198515892853865), (0, 244, 0.09198515892853865), (0, 243, 0.09198515892853865), (0, 206, 0.09198515892853865), (0, 166, 0.09198515892853865), (0, 159, 0.09198515892853865), (0, 158, 0.09198515892853865), (0, 99, 0.09198515892853865), (0, 80, 0.09198515892853865), (0, 43, 0.09198515892853865), (0, 13, 0.09198515892853865), (0, 335, 0.09043407314997833), (0, 296, 0.09043407314997833), (0, 458, 0.08178139787814755), (0, 382, 0.08178139787814755), (0, 361, 0.08178139787814755), (0, 179, 0.08178139787814755), (0, 170, 0.08178139787814755), (0, 169, 0.08178139787814755), (0, 165, 0.08178139787814755), (0, 128, 0.08178139787814755), (0, 435, 0.0751284315743917), (0, 280, 0.0751284315743917), (0, 59, 0.0751284315743917), (0, 505, 0.07344431805260855), (0, 385, 0.07344431805260855), (0, 320, 0.07344431805260855), (0, 262, 0.07344431805260855), (0, 210, 0.07344431805260855), (0, 140, 0.07344431805260855), (0, 111, 0.07344431805260855), (0, 82, 0.07344431805260855), (0, 500, 0.06639541749756694), (0, 487, 0.06639541749756694), (0, 383, 0.06639541749756694), (0, 345, 0.06639541749756694), (0, 215, 0.06639541749756694), (0, 147, 0.06639541749756694), (0, 142, 0.06639541749756694), (0, 92, 0.06639541749756694), (0, 57, 0.06639541749756694), (0, 239, 0.05490347717667846), (0, 517, 0.05257004744058082), (0, 516, 0.05257004744058082), (0, 513, 0.05257004744058082), (0, 507, 0.05257004744058082), (0, 498, 0.05257004744058082), (0, 494, 0.05257004744058082), (0, 489, 0.05257004744058082), (0, 485, 0.05257004744058082), (0, 476, 0.05257004744058082), (0, 465, 0.05257004744058082), (0, 440, 0.05257004744058082), (0, 418, 0.05257004744058082), (0, 416, 0.05257004744058082), (0, 409, 0.05257004744058082), (0, 399, 0.05257004744058082), (0, 370, 0.05257004744058082), (0, 364, 0.05257004744058082), (0, 343, 0.05257004744058082), (0, 337, 0.05257004744058082), (0, 328, 0.05257004744058082), (0, 313, 0.05257004744058082), (0, 304, 0.05257004744058082), (0, 303, 0.05257004744058082), (0, 298, 0.05257004744058082), (0, 291, 0.05257004744058082), (0, 283, 0.05257004744058082), (0, 277, 0.05257004744058082), (0, 275, 0.05257004744058082), (0, 269, 0.05257004744058082), (0, 267, 0.05257004744058082), (0, 260, 0.05257004744058082), (0, 254, 0.05257004744058082), (0, 238, 0.05257004744058082), (0, 231, 0.05257004744058082), (0, 229, 0.05257004744058082), (0, 202, 0.05257004744058082), (0, 199, 0.05257004744058082), (0, 191, 0.05257004744058082), (0, 184, 0.05257004744058082), (0, 183, 0.05257004744058082), (0, 175, 0.05257004744058082), (0, 168, 0.05257004744058082), (0, 167, 0.05257004744058082), (0, 164, 0.05257004744058082), (0, 138, 0.05257004744058082), (0, 124, 0.05257004744058082), (0, 118, 0.05257004744058082), (0, 116, 0.05257004744058082), (0, 100, 0.05257004744058082), (0, 93, 0.05257004744058082), (0, 78, 0.05257004744058082), (0, 61, 0.05257004744058082), (0, 40, 0.05257004744058082), (0, 34, 0.05257004744058082), (0, 21, 0.05257004744058082), (0, 8, 0.05257004744058082), (0, 3, 0.05257004744058082), (0, 436, 0.05008562104959446), (0, 515, 0.04599257946426932), (0, 514, 0.04599257946426932), (0, 512, 0.04599257946426932), (0, 497, 0.04599257946426932), (0, 495, 0.04599257946426932), (0, 493, 0.04599257946426932), (0, 468, 0.04599257946426932), (0, 451, 0.04599257946426932), (0, 445, 0.04599257946426932), (0, 441, 0.04599257946426932), (0, 390, 0.04599257946426932), (0, 376, 0.04599257946426932), (0, 373, 0.04599257946426932), (0, 368, 0.04599257946426932), (0, 360, 0.04599257946426932), (0, 359, 0.04599257946426932), (0, 354, 0.04599257946426932), (0, 325, 0.04599257946426932), (0, 307, 0.04599257946426932), (0, 306, 0.04599257946426932), (0, 282, 0.04599257946426932), (0, 230, 0.04599257946426932), (0, 228, 0.04599257946426932), (0, 226, 0.04599257946426932), (0, 216, 0.04599257946426932), (0, 205, 0.04599257946426932), (0, 203, 0.04599257946426932), (0, 189, 0.04599257946426932), (0, 176, 0.04599257946426932), (0, 145, 0.04599257946426932), (0, 88, 0.04599257946426932), (0, 69, 0.04599257946426932), (0, 68, 0.04599257946426932), (0, 56, 0.04599257946426932), (0, 44, 0.04599257946426932), (0, 484, 0.04089069893907377), (0, 300, 0.04089069893907377), (0, 299, 0.04089069893907377), (0, 295, 0.04089069893907377), (0, 264, 0.04089069893907377), (0, 246, 0.04089069893907377), (0, 240, 0.04089069893907377), (0, 224, 0.04089069893907377), (0, 194, 0.04089069893907377), (0, 123, 0.04089069893907377), (0, 117, 0.04089069893907377), (0, 105, 0.04089069893907377), (0, 76, 0.04089069893907377), (0, 52, 0.04089069893907377), (0, 45, 0.04089069893907377), (0, 7, 0.04089069893907377), (0, 504, 0.036722159026304275), (0, 473, 0.036722159026304275), (0, 457, 0.036722159026304275), (0, 442, 0.036722159026304275), (0, 426, 0.036722159026304275), (0, 346, 0.036722159026304275), (0, 302, 0.036722159026304275), (0, 286, 0.036722159026304275), (0, 278, 0.036722159026304275), (0, 241, 0.036722159026304275), (0, 98, 0.036722159026304275), (0, 96, 0.036722159026304275), (0, 77, 0.036722159026304275), (0, 27, 0.036722159026304275), (0, 22, 0.036722159026304275), (0, 511, 0.03319770874878347), (0, 510, 0.03319770874878347), (0, 469, 0.03319770874878347), (0, 463, 0.03319770874878347), (0, 456, 0.03319770874878347), (0, 422, 0.03319770874878347), (0, 330, 0.03319770874878347), (0, 285, 0.03319770874878347), (0, 263, 0.03319770874878347), (0, 218, 0.03319770874878347), (0, 204, 0.03319770874878347), (0, 141, 0.03319770874878347), (0, 136, 0.03319770874878347), (0, 58, 0.03319770874878347), (0, 50, 0.03319770874878347), (0, 4, 0.03319770874878347), (0, 472, 0.030144691049992775), (0, 437, 0.030144691049992775), (0, 431, 0.030144691049992775), (0, 292, 0.030144691049992775), (0, 127, 0.030144691049992775), (0, 47, 0.030144691049992775), (0, 5, 0.030144691049992775), (0, 144, 0.02745173858833923), (0, 20, 0.02745173858833923)]\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformer.fit(word_counts)\n",
    "\n",
    "job_info = descriptions[0]\n",
    "\n",
    "count_vector = cv.transform([job_info])\n",
    "\n",
    "tfidf_vector = transformer.transform(count_vector).tocoo()\n",
    "tuples = zip(tfidf_vector.row, tfidf_vector.col, tfidf_vector.data)\n",
    "tfidf_vector = sorted(tuples, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(tfidf_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "live 0.3219\n",
      "technical 0.2045\n",
      "stakeholders 0.1809\n",
      "time 0.1469\n",
      "real 0.138\n",
      "people 0.1252\n",
      "needs 0.1227\n",
      "support 0.1206\n",
      "requirements 0.1102\n",
      "learn 0.1102\n",
      "insights 0.1102\n",
      "committed 0.1102\n",
      "skills 0.1098\n",
      "successfully 0.1051\n",
      "rely 0.1051\n",
      "privacy 0.1051\n",
      "largest 0.1051\n",
      "intern 0.1051\n",
      "deployment 0.1051\n",
      "world 0.0996\n"
     ]
    }
   ],
   "source": [
    "top_keywords = tfidf_vector[:20]\n",
    "\n",
    "for tup in top_keywords:\n",
    "    print(features[tup[1]], round(tup[2], 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
